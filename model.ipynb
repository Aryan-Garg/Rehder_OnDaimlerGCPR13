{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Understanding how Markovian models perform for autonomous driving(perception) purposes </center></h1>\n",
    "\n",
    "### Paper: Goal Directed Pedestrian Prediction. Eike Rehder, ICCV15 \n",
    "### Dataset: Daimler Dataset GCPR13 \n",
    "\n",
    "#### Implemented by:\n",
    "> Aryan Garg  \n",
    "> Dr. Amit S. Unde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+]Loading essential packages...\n",
      "[+]Loaded OpenCV version 4.5.2 @ Tue Sep 21 22:22:29 2021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For all modules that produce a not-found / needs update error:\n",
    "# In the notebook, before importing, write after\n",
    "# Replacing angular brackets with module name:\n",
    "# ! pip install <package-name-as-per-package's-documentation>\n",
    "\n",
    "import cv2 \n",
    "from PIL import Image\n",
    "import imutils\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "\n",
    "\n",
    "print(f\"[+]Loading essential packages...\")\n",
    "print(f\"[+]Loaded OpenCV version {cv2.__version__} @ {time.asctime(time.localtime(time.time()))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preProcessData:\n",
    "    \n",
    "    def readData(self, trainDataP):\n",
    "        try:\n",
    "            dirs = [f for f in listdir(trainDataP)]\n",
    "            for directory in dirs:\n",
    "                newPath = self.trainDataPath + directory + \"\\\\RectGrabber\\\\\"\n",
    "                files = [f for f in listdir(newPath) if isfile(join(newPath, f))]\n",
    "                self.allTrainingData[directory] = files\n",
    "            #print(self.allTrainingData)\n",
    "            return True\n",
    "        \n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            print(\"Couldn't read data. Check file paths and file health!\")\n",
    "            return False\n",
    "        \n",
    "    \n",
    "    \n",
    "    def showFrames(self, windowName, imglst):\n",
    "        if len(windowName) != len(imglst):\n",
    "            print(f\"windowName list len: {len(windowName)} not equal to imglst len: {len(imglst)}\")\n",
    "            return False \n",
    "        \n",
    "        for i in range(len(windowName)):\n",
    "            cv2.imshow(f\"{windowName[i]}\", imglst[i])\n",
    "        \n",
    "        k = cv2.waitKey(0)\n",
    "        if k is not None:\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    def showSampleImages(self, folderName = \"2012-06-05_165931\"):\n",
    "        ### TODO: Decrease Image size\n",
    "        print(\"[+] Logging sample images' details\\n---------------------\")\n",
    "        for i in range(1,10,2): \n",
    "            img_left = self.allTrainingData[folderName][i-1]\n",
    "            img_right = self.allTrainingData[folderName][i]\n",
    "\n",
    "            try:\n",
    "                imgL = cv2.imread(self.trainDataPath + folderName + \"\\\\RectGrabber\\\\\" + img_left)\n",
    "                imgR = cv2.imread(self.trainDataPath + folderName + \"\\\\RectGrabber\\\\\" + img_right)\n",
    "                \n",
    "                imgL = cv2.resize(imgL, (600,450))\n",
    "                imgR = cv2.resize(imgR, (600,450))\n",
    "                print(f\"{(i//2) + 1}. Image names: {img_left} & {img_right}\\n\\t\\tShape-L: {imgL.shape}     Shape-R: {imgR.shape}\\n\")\n",
    "                \n",
    "                if not self.showFrames([\"Left frame\", \"Right frame\"], [imgL, imgR]):\n",
    "                    print(f\"Couldn't show sample images from showSampleImages function\")\n",
    "            \n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                if imgL is None:\n",
    "                    print(f\"[!]Couldn't load L-image: {img_left}\")\n",
    "                if imgR is None:\n",
    "                    print(f\"[!]Couldn't load R-image: {img_right}\")\n",
    "                continue\n",
    "                \n",
    "        print(\"---------------------\\n[+] Finished viewing initial samples.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    def frameFromLR(self):\n",
    "        ### TODO: Will have to refer to the 14th paper later\n",
    "        ### For now standard\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def removeNoise(self):\n",
    "        ### TODO: Future optimization\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def createVideo(self, folderNum):\n",
    "        try:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "            video = cv2.VideoWriter('out_video.avi', fourcc, 30.0, self.allTrainingData[folderNum][0].shape)\n",
    "        \n",
    "            for imgL, imgR in self.allTrainingData[folderNum][:121]:\n",
    "                video.write(imgL)\n",
    "\n",
    "            video.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(f\"[+]Video out_video.avi released\")\n",
    "            \n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            print(f\"Possible fixes:\\n1. See if DIVX is compatible with your machine\\n2. Use a media player that supports .avi\\n\\\n",
    "                    3. Match the shape of frames with the video\")\n",
    "\n",
    "    def detectPedestriansAnnotations(self, dirName, imageList):\n",
    "        '''\n",
    "            \n",
    "            Brief: Making bounding boxes using the annotations provided in dataset\n",
    "            \n",
    "            Need to use SQL queries here to extract info!!! (.db files are present)\n",
    "            \n",
    "        '''\n",
    "        print(\"\\nCreating Bounding Boxes...\")\n",
    "        print(f\"\\nDRAWING FOR {dirName}\")\n",
    "        path_ = 'C:\\\\Users\\\\HP\\\\Desktop\\Research\\\\Trajectory_Markov_Research\\\\Implementations\\\\Dataset_Dailmer\\\\TrainingData_Annotations\\\\'\n",
    "        path_ += str(dirName)+\"\\\\LabelData\\\\\"\n",
    "        # creating file path\n",
    "        dbfile = path_ + \"meas.db\"\n",
    "        \n",
    "        toRet = []\n",
    "        with open(dbfile) as f:\n",
    "            \n",
    "            rd = f.readlines()\n",
    "            # Text formatting params:\n",
    "            font                   = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "            fontScale              = 0.7\n",
    "            fontColor              = (255, 255, 255)\n",
    "            lineType               = 2\n",
    "            for i in range(len(rd)):\n",
    "                imgName = \"\"\n",
    "                box_cos = \"\"\n",
    "                \n",
    "                if \"img\" in rd[i]:\n",
    "                    imgName = rd[i][:-1]\n",
    "                    \n",
    "                    if i+6 < len(rd):\n",
    "                        box_cos = rd[i+6].split()\n",
    "                        if len(box_cos) != 4:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    imgName = \"imgrect_\"+imgName[4:] \n",
    "                    #print(f\"{imgName}:{box_cos}\")\n",
    "                    \n",
    "                    img = cv2.imread( \"C:\\\\Users\\\\HP\\\\Desktop\\\\Research\\\\Trajectory_Markov_Research\\\\Implementations\\Dataset_Dailmer\\\\Data\\\\TrainingData\\\\\" \n",
    "                                     + dirName + \"\\\\RectGrabber\\\\\" + imgName )\n",
    "                    \n",
    "                    if img is None:\n",
    "                        print(\"[-]Image could not load!\")\n",
    "                        continue\n",
    "                \n",
    "                    \n",
    "                    box_cos = [int(e) for e in box_cos]\n",
    "                    x,y,a,b = box_cos # Top left: (x,y) ; Bottom Right: (a,b)\n",
    "                    \n",
    "                    cv2.rectangle(img, (x, y), (a,b), (0, 255, 0), 2)\n",
    "                    #cv2.putText(img,'Person', (int(x),int(y)), font, fontScale,fontColor,lineType)\n",
    "                    \n",
    "                    #cv2.imshow(\"Bounding Box from Annotations\", img)\n",
    "                    #cv2.waitKey(0)\n",
    "                    \n",
    "                    toRet.append(img)\n",
    "            \n",
    "            f.close()\n",
    "            return toRet\n",
    "\n",
    "    \n",
    "    def detectPedestriansHOG(self, dirName, imageList): \n",
    "        '''\n",
    "            Brief: \n",
    "                    Standard Histogram Oriented Gradients Object Detection provided by openCV. \n",
    "                    (Dalal & Triggs)\n",
    "            |\n",
    "            Param:\n",
    "                    frame -> For which pedestrian detection must be done\n",
    "                    \n",
    "            Returns:\n",
    "                    Frame with a green bounding box around pedestrians\n",
    "        '''\n",
    "        \n",
    "        toRet = []             # This list will become the object detected data list's part\n",
    "        \n",
    "        # Text formatting params:\n",
    "        font                   = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "        fontScale              = 0.7\n",
    "        fontColor              = (255, 255, 255)\n",
    "        lineType               = 2\n",
    "        \n",
    "        # Initialize standard HOG people detector\n",
    "        hog = cv2.HOGDescriptor()\n",
    "        hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "        \n",
    "        for imgName in imageList:\n",
    "            img = cv2.imread( self.trainDataPath + dirName + \"\\\\RectGrabber\\\\\" + imgName )\n",
    "            \n",
    "            if img is None:\n",
    "                continue\n",
    "                \n",
    "            img = cv2.resize(img, (600, 450))\n",
    "            (rects, weights) = hog.detectMultiScale(img, winStride=(4, 4), padding=(8, 8), scale=1.1)\n",
    "            \n",
    "            for (x, y, w, h) in rects:\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(img,'Person', (x,y), font, fontScale,fontColor,lineType)\n",
    "            \n",
    "            toRet.append(img)\n",
    "                \n",
    "            #if not self.showFrames([\"Bounding Box\"], [img]):\n",
    "             #   print(f\"Couldn't show pedestrian detected image: {img}\")\n",
    "            \n",
    "        \n",
    "        return toRet\n",
    "           \n",
    "    \n",
    "    def __init__(self):\n",
    "        ### Change filepath according to your machine config\n",
    "        self.trainDataPath = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Research\\\\Trajectory_Markov_Research\\\\Implementations\\Dataset_Dailmer\\\\Data\\\\TrainingData\\\\\"\n",
    "        self.allTrainingData = dict()\n",
    "        if self.readData(self.trainDataPath):\n",
    "            self.showSampleImages()\n",
    "            self.detectedData = {}\n",
    "            for key in self.allTrainingData:\n",
    "                self.detectedData[key] = self.detectPedestriansAnnotations(key, self.allTrainingData[key])\n",
    "            \n",
    "            for key in self.detectedData:\n",
    "                for image in self.detectedData[key]:\n",
    "                    cv2.imshow(\"Bounding Box\", image)\n",
    "                    cv2.waitKey(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    '''\n",
    "    Brief:\n",
    "        Goal Directed Pedestrian Prediction \n",
    "        (A markovian approach for pedestrian trajectory prediction)\n",
    "        \n",
    "    Receives:\n",
    "        Pre-processed and pedestrain detected data from preProcessData class\n",
    "        \n",
    "    Returns:\n",
    "        Predicted Trajectory of each pedestrian in scene\n",
    "                \n",
    "    '''\n",
    "    \n",
    "    # State Transition stuff ahead:\n",
    "    def current_speed(self):\n",
    "        # Returns instantaneous v_x, v_y\n",
    "        # 2 frames: t and t-1\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def view30Frames(self, key):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def eq1_positionTransition(self):\n",
    "        self.X.append(X[-1] + self.eq2_unicycle())\n",
    "        self.eq3()\n",
    "    \n",
    "    def eq2_unicyle(self):\n",
    "        vx , vy = self.current_speed()\n",
    "        return 1/self.fps * (vx, vy, 0)\n",
    "    \n",
    "    \n",
    "    def eq3(self):\n",
    "        '''\n",
    "            phi_t = p( X_t | X_t-1 )\n",
    "            p(Xt|Xt−1) = p(Xt−1) ⊗ p(u(vt, ψt))\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def discretization_2_grid(self):\n",
    "        '''\n",
    "            Consider 2x2 cells -> assign probability(from incremental distribution) to each cell\n",
    "            Returns: A (the convolution filter mask) \n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def eq4(self):\n",
    "        '''\n",
    "        Model the incremental distribution:\n",
    "        \n",
    "            p(∆x, ∆y, ∆ψ) ∝  exp(−(∆x−∆tv cos(ψ))^2/2σ_v^2)\n",
    "                            .exp(−(∆y−∆tv sin(ψ))^2/2σ_v^2)\n",
    "                            .exp(κ∆ψ cos(∆ψ))\n",
    "                            .exp(κv cos(∠(∆y, ∆x) − ψ))\n",
    "                            \n",
    "        Then call self.discretization_2_grid() to get A (the convolution filter mask)\n",
    "        Then call eq5 to get phi_t using A and phi_t-1\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def eq5(self):\n",
    "        '''\n",
    "            Φt ∝ A ⊗ Φt−1\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def eq6(self):\n",
    "        '''\n",
    "            Invert distribution in 4 to get (B)           [A^(-1)]\n",
    "            *** For backward prediction steps from XT ***\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def eq7(self):\n",
    "        '''\n",
    "            Carry out forward and backward steps to get each p(Xt | X0, XT) acc. to:\n",
    "            p(Xt|X0, XT ) ∝ Φ+t Φ−t\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # Location Prior stuff ahead\n",
    "    def eq8(self):\n",
    "        '''\n",
    "            Forward step:\n",
    "            Φ+t ∝ p(Xt|Θt)(A ⊗ Φ+t−1)\n",
    "            \n",
    "            where Θt is the online map\n",
    "                  p(X|θi) is the location prior (call eq10 here)\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def eq9(self):\n",
    "        '''\n",
    "            Backward step:\n",
    "            Φ−t−1 ∝ p(Xt|Θt)(A^(−1) ⊗ Φ−t)\n",
    "            \n",
    "            call eq10 from here\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def eq10(self):\n",
    "        '''\n",
    "            Computes Location prior: p(X|θi) acc to:\n",
    "            \n",
    "            Single Layer Perceptron: \n",
    "            p(X|θi) = 1 / 1 + exp (−a^T . θ_i)\n",
    "            \n",
    "            where a^T represents weighting params for different features. \n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def eq11(self):\n",
    "        '''\n",
    "            Path Distribution: \n",
    "            p(XM|X0, XT , Θt) = 1 −PI[(M) <-- (t˜=0)](1 − p(Xt˜|X0, XT , Θt))\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def eq12(self):\n",
    "        '''\n",
    "            Cost function to get best weighting params, i.e, a:\n",
    "            J(a) = −Summation[ ζi ∈ Xj ] Summation[ Xj ∈ ζi ] log(p(X = Xj |X0, XT , Θt))\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    # Goal Distribution XT Estimation ahead\n",
    "    def eq13(self):\n",
    "        '''\n",
    "            p(Xt) ::= estimated distribution for the pedestrian’s state at time = t. \n",
    "            p(X−t|X0, XT , Θt) ::= the distribution of that state obtained from prediction at t-1. \n",
    "        \n",
    "            [!] After Assumption: Independence\n",
    "            Can obtain:\n",
    "        \n",
    "                pX− (X−t|X0, XT , Θt) ∝ pX− (X0, XT , Θt|X−t)p(X−t)\n",
    "        \n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def eq14(self):\n",
    "        '''\n",
    "            p(XT ) ∝ Integral of pX− (X0, XT , Θt|Xt).p(Xt).dX\n",
    "            \n",
    "            [+] Evaluated for the individual particles and the result is used for reweighting\n",
    "            Next TODO: Unlikely particles are discarded and randomly resampled at other locations.\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __init__(self, processedData):\n",
    "        self.data = processedData\n",
    "        self.fps = 30                          # fps\n",
    "        self.X = []                            # Contains last t-1 positions & orientations: (x_t, y_t, psi_t)\n",
    "        self.px0 = 0\n",
    "        self.XT = None                         # Latent Variable: Gaussian Mixture Model + Particle Filter\n",
    "        self.currentFrame = None\n",
    "        for key in self.data:\n",
    "            self.pX0 = self.view30Frames(key)  # Watch 1 sec to estimate initial params\n",
    "            self.currentFrame = self.data[key][30]\n",
    "            self.eq1_positionTransition()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class analytics:\n",
    "    '''\n",
    "    Brief:\n",
    "        Analyzes predicted trajectories against ground truth.\n",
    "        \n",
    "    Receives:\n",
    "        Predicted trajectories from model class\n",
    "        \n",
    "    Displays:\n",
    "        Accuracy and evaluation metrics. And other visual-aids for analysis of the model\n",
    "                \n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Logging sample images' details\n",
      "---------------------\n",
      "1. Image names: imgrect_000000000_c0.pgm & imgrect_000000000_c1.pgm\n",
      "\t\tShape-L: (450, 600, 3)     Shape-R: (450, 600, 3)\n",
      "\n",
      "2. Image names: imgrect_000000001_c0.pgm & imgrect_000000001_c1.pgm\n",
      "\t\tShape-L: (450, 600, 3)     Shape-R: (450, 600, 3)\n",
      "\n",
      "3. Image names: imgrect_000000002_c0.pgm & imgrect_000000002_c1.pgm\n",
      "\t\tShape-L: (450, 600, 3)     Shape-R: (450, 600, 3)\n",
      "\n",
      "4. Image names: imgrect_000000003_c0.pgm & imgrect_000000003_c1.pgm\n",
      "\t\tShape-L: (450, 600, 3)     Shape-R: (450, 600, 3)\n",
      "\n",
      "5. Image names: imgrect_000000004_c0.pgm & imgrect_000000004_c1.pgm\n",
      "\t\tShape-L: (450, 600, 3)     Shape-R: (450, 600, 3)\n",
      "\n",
      "---------------------\n",
      "[+] Finished viewing initial samples.\n",
      "FOR 2012-04-02_115542\n",
      "FOR 2012-04-02_115639_Short\n",
      "FOR 2012-04-02_120604\n",
      "FOR 2012-04-02_122116\n",
      "FOR 2012-04-13_135615_Short\n",
      "FOR 2012-04-13_135852\n",
      "FOR 2012-04-13_140130_Short\n",
      "FOR 2012-06-05_165701\n",
      "FOR 2012-06-05_165931\n",
      "FOR 2012-06-21_143954\n",
      "FOR 2012-06-21_144420\n",
      "FOR 2012-06-21_145358\n",
      "FOR 2012-06-21_150408\n",
      "FOR 2012-10-11_134955\n",
      "FOR 2012-10-11_135232\n",
      "FOR 2012-10-11_135436\n",
      "FOR 2012-10-11_135800\n",
      "FOR 2012-10-11_135934\n",
      "FOR 2012-10-11_140519\n",
      "FOR 2012-10-11_140717\n",
      "FOR 2012-10-11_140822\n",
      "FOR 2012-10-11_143031\n",
      "FOR 2012-10-11_143231\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-3c99994dded3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create instances here :)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreProcessData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;31m# Remove noise, use paper 14, Detect pedestrians(HOG) and then pass to model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(df.detectedData)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#predicted_df= model(df.processedData)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-1e42a7147500>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectedData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallTrainingData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectedData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectPedestriansAnnotations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallTrainingData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallTrainingData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-1e42a7147500>\u001b[0m in \u001b[0;36mdetectPedestriansAnnotations\u001b[1;34m(self, dirName, imageList)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                     img = cv2.imread( \"C:\\\\Users\\\\HP\\\\Desktop\\\\Research\\\\Trajectory_Markov_Research\\\\Implementations\\Dataset_Dailmer\\\\Data\\\\TrainingData\\\\\" \n\u001b[1;32m--> 133\u001b[1;33m                                      + dirName + \"\\\\RectGrabber\\\\\" + imgName )\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create instances here :)\n",
    "\n",
    "df = preProcessData()                       # Remove noise, use paper 14, Detect pedestrians(HOG) and then pass to model\n",
    "\n",
    "import json\n",
    "with open('annotatedData.json', 'w') as fp:\n",
    "    json.dump(df.detectedData, fp)\n",
    "    \n",
    "#print(df.detectedData)\n",
    "#predicted_df= model(df.processedData)\n",
    "#analytics(predicted_df.preds)\n",
    "\n",
    "#if img.shape != (64, 128):\n",
    "#    img = cv2.resize(img, (64, 128))\n",
    "\n",
    "'''\n",
    "img = cv2.imread(\"C:\\\\Users\\\\HP\\\\Desktop\\\\Research\\\\Trajectory_Markov_Research\\\\Implementations\\Dataset_Dailmer\\\\Data\\\\TrainingData\\\\2012-04-02_115542\\\\RectGrabber\\\\imgrect_000000000_c0.pgm\")\n",
    "cv2.imshow(\"test\", img)\n",
    "\n",
    "cv2.rectangle(img, (764,227), (792,303),  (0, 255, 0), 2)\n",
    "cv2.imshow(\"test_box\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
